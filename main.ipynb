{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"images\"\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_in_folder(folder_path, target_size):\n",
    "    # Create a new folder to store resized images\n",
    "    resized_folder_path = os.path.join(folder_path, \"resized\")\n",
    "    os.makedirs(resized_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Iterate through each image\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Check if the file is an image (JPEG or PNG)\n",
    "        if os.path.isfile(file_path) and (file_name.lower().endswith('.jpg') or file_name.lower().endswith('.png')):\n",
    "            # Load and resize the image\n",
    "            image = cv2.imread(file_path)\n",
    "            resized_image = cv2.resize(image, target_size)\n",
    "            \n",
    "            # Save the resized image to the new folder\n",
    "            resized_image_path = os.path.join(resized_folder_path, file_name)\n",
    "            cv2.imwrite(resized_image_path, resized_image)\n",
    "            # print(f\"Resized image saved: {resized_image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing complete!\n"
     ]
    }
   ],
   "source": [
    "resize_images_in_folder(folder_path, target_size)\n",
    "\n",
    "print(\"Resizing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Function to generate darker images\n",
    "def generate_darker_images(input_folder, output_folder, factor, apply_probability=0.25):\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through images in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            # Read image\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Randomly decide whether to apply darker effect\n",
    "            if random.random() < apply_probability:\n",
    "                # Apply darker effect\n",
    "                darker_img = cv2.convertScaleAbs(img, alpha=factor, beta=0)\n",
    "            else:\n",
    "                # No change, keep the original image\n",
    "                darker_img = img.copy()\n",
    "            \n",
    "            # Save image\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, darker_img)\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"input_images_folder\"\n",
    "output_folder = \"output_images_folder\"\n",
    "darkening_factor = 0.5  # Adjust this factor as needed, lower values make the image darker\n",
    "\n",
    "generate_darker_images('images/resized', 'darkened', darkening_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 1125.jpg\n",
      "Processed: 1139.jpg\n",
      "Processed: 1143.jpg\n",
      "Processed: 115.jpg\n",
      "Processed: 1173.jpg\n",
      "Processed: 1175.jpg\n",
      "Processed: 1179.jpg\n",
      "Processed: 1186.jpg\n",
      "Processed: 1192.jpg\n",
      "Processed: 1319.jpg\n",
      "Processed: 1360.jpg\n",
      "Processed: 1372.jpg\n",
      "Processed: 1377.jpg\n",
      "Processed: 1428.jpg\n",
      "Processed: 1447.jpg\n",
      "Processed: 1495.jpg\n",
      "Processed: 1510.jpg\n",
      "Processed: 1577.jpg\n",
      "Processed: 1590.jpg\n",
      "Processed: 1617.jpg\n",
      "Processed: 1937.jpg\n",
      "Processed: 1957.jpg\n",
      "Processed: 1969.jpg\n",
      "Processed: 277.jpg\n",
      "Processed: 2970.jpg\n",
      "Processed: 306.jpg\n",
      "Processed: 316.jpg\n",
      "Processed: 319.jpg\n",
      "Processed: 327.jpg\n",
      "Processed: 331.jpg\n",
      "Processed: 3427.jpg\n",
      "Processed: 347.jpg\n",
      "Processed: 3578.jpg\n",
      "Processed: 368.jpg\n",
      "Processed: 382.jpg\n",
      "Processed: 405.jpg\n",
      "Processed: 417.jpg\n",
      "Processed: 419.jpg\n",
      "Processed: 4272.jpg\n",
      "Processed: 429.jpg\n",
      "Processed: 4357.jpg\n",
      "Processed: 437.jpg\n",
      "Processed: 4439.jpg\n",
      "Processed: 4470.jpg\n",
      "Processed: 450.jpg\n",
      "Processed: 4563.jpg\n",
      "Processed: 469.jpg\n",
      "Processed: 4786.jpg\n",
      "Processed: 4932.jpg\n",
      "Processed: 496.jpg\n",
      "Processed: 503.jpg\n",
      "Processed: 512.jpg\n",
      "Processed: 5176.jpg\n",
      "Processed: 521.jpg\n",
      "Processed: 5214.jpg\n",
      "Processed: 5230.jpg\n",
      "Processed: 5298.jpg\n",
      "Processed: 5315.jpg\n",
      "Processed: 5340.jpg\n",
      "Processed: 5356.jpg\n",
      "Processed: 5525.jpg\n",
      "Processed: 5602.jpg\n",
      "Processed: 5608.jpg\n",
      "Processed: 5632.jpg\n",
      "Processed: 5644.jpg\n",
      "Processed: 5647.jpg\n",
      "Processed: 5650.jpg\n",
      "Processed: 5663.jpg\n",
      "Processed: 5666.jpg\n",
      "Processed: 5671.jpg\n",
      "Processed: 5672.jpg\n",
      "Processed: 5680.jpg\n",
      "Processed: 5686.jpg\n",
      "Processed: 5691.jpg\n",
      "Processed: 5694.jpg\n",
      "Processed: 5742.jpg\n",
      "Processed: 5750.jpg\n",
      "Processed: 5754.jpg\n",
      "Processed: 5760.jpg\n",
      "Processed: 5806.jpg\n",
      "Processed: 586.jpg\n",
      "Processed: 5946.jpg\n",
      "Processed: 5957.jpg\n",
      "Processed: 5967.jpg\n",
      "Processed: 6011.jpg\n",
      "Processed: 6031.jpg\n",
      "Processed: 6062.jpg\n",
      "Processed: 6074.jpg\n",
      "Processed: 6162.jpg\n",
      "Processed: 6177.jpg\n",
      "Processed: 6179.jpg\n",
      "Processed: 6190.jpg\n",
      "Processed: 6214.jpg\n",
      "Processed: 6225.jpg\n",
      "Processed: 6278.jpg\n",
      "Processed: 6345.jpg\n",
      "Processed: 6356.jpg\n",
      "Processed: 6372.jpg\n",
      "Processed: 6373.jpg\n",
      "Processed: 6404.jpg\n",
      "Processed: 656.jpg\n",
      "Processed: 668.jpg\n",
      "Processed: 673.jpg\n",
      "Processed: 675.jpg\n",
      "Processed: 683.jpg\n",
      "Processed: 695.jpg\n",
      "Processed: 703.jpg\n",
      "Processed: 707.jpg\n",
      "Processed: 708.jpg\n",
      "Processed: 710.jpg\n",
      "Processed: 712.jpg\n",
      "Processed: 715.jpg\n",
      "Processed: 716.jpg\n",
      "Processed: 723.jpg\n",
      "Processed: 727.jpg\n",
      "Processed: 728.jpg\n",
      "Processed: 735.jpg\n",
      "Processed: 737.jpg\n",
      "Processed: 750.jpg\n",
      "Processed: 751.jpg\n",
      "Processed: 752.jpg\n",
      "Processed: 754.jpg\n",
      "Processed: 755.jpg\n",
      "Processed: 756.jpg\n",
      "Processed: 760.jpg\n",
      "Processed: 763.jpg\n",
      "Processed: 764.jpg\n",
      "Processed: 771.jpg\n",
      "Processed: 778.jpg\n",
      "Processed: 779.jpg\n",
      "Processed: 784.jpg\n",
      "Processed: 785.jpg\n",
      "Processed: 786.jpg\n",
      "Processed: 795.jpg\n",
      "Processed: 796.jpg\n",
      "Processed: 797.jpg\n",
      "Processed: 800.jpg\n",
      "Processed: 804.jpg\n",
      "Processed: 807.jpg\n",
      "Processed: 808.jpg\n",
      "Processed: 809.jpg\n",
      "Processed: 814.jpg\n",
      "Processed: 815.jpg\n",
      "Processed: 816.jpg\n",
      "Processed: 820.jpg\n",
      "Processed: 825.jpg\n",
      "Processed: 827.jpg\n",
      "Processed: 834.jpg\n",
      "Processed: 839.jpg\n",
      "Processed: 852.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for directory - hair removal (quality compromise))\n",
    "\n",
    "def process_images_with_hair(input_folder, output_folder):\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through images in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            # Read image\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            src = cv2.imread(img_path)\n",
    "\n",
    "            # Convert the original image to grayscale\n",
    "            grayScale = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Kernel for the morphological filtering\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 17))\n",
    "\n",
    "            # Perform the blackHat filtering on the grayscale image to find the hair contours\n",
    "            blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "            # Intensify the hair contours in preparation for the inpainting algorithm\n",
    "            _, thresh2 = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Inpaint the original image depending on the mask\n",
    "            dst = cv2.inpaint(src, thresh2, 1, cv2.INPAINT_TELEA)\n",
    "\n",
    "            # Save the processed image\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, dst)\n",
    "\n",
    "            print(f\"Processed: {filename}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"darkened\"\n",
    "output_folder = \"finalpreprocessed\"\n",
    "\n",
    "process_images_with_hair(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(224, 224)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "src = cv2.imread(\"images/795.jpg\")\n",
    "\n",
    "print( src.shape )\n",
    "cv2.imshow(\"original Image\" , src )\n",
    "\n",
    "\n",
    "# Convert the original image to grayscale\n",
    "grayScale = cv2.cvtColor( src, cv2.COLOR_RGB2GRAY )\n",
    "cv2.imshow(\"GrayScale\",grayScale)\n",
    "cv2.imwrite('grayScale_sample1.jpg', grayScale, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
    "\n",
    "# Kernel for the morphological filtering\n",
    "kernel = cv2.getStructuringElement(1,(17,17))\n",
    "\n",
    "# Perform the blackHat filtering on the grayscale image to find the \n",
    "# hair countours\n",
    "blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "cv2.imshow(\"BlackHat\",blackhat)\n",
    "cv2.imwrite('blackhat_sample1.jpg', blackhat, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
    "\n",
    "# intensify the hair countours in preparation for the inpainting \n",
    "# algorithm\n",
    "ret,thresh2 = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n",
    "print( thresh2.shape )\n",
    "cv2.imshow(\"Thresholded Mask\",thresh2)\n",
    "cv2.imwrite('thresholded_sample1.jpg', thresh2, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
    "\n",
    "# inpaint the original image depending on the mask\n",
    "dst = cv2.inpaint(src,thresh2,1,cv2.INPAINT_TELEA)\n",
    "cv2.imshow(\"InPaint\",dst)\n",
    "cv2.imwrite('ot3.jpg', dst, [int(cv2.IMWRITE_JPEG_QUALITY),90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##GAN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),#second changed to 7 from 4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),  # Changed output channels to 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GAN\n",
    "latent_dim = 100\n",
    "channels = 3  # RGB channels\n",
    "img_size = 64  # Desired image size    64 to 224 change\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(latent_dim, channels)\n",
    "discriminator = Discriminator(channels)\n",
    "\n",
    "# Set up the loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Load your skin lesion dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*channels, [0.5]*channels)\n",
    "])\n",
    "\n",
    "dataset = SkinLesionDataset('finalpreprocessed', transform=transform)\n",
    "total_images = len(dataset)\n",
    "\n",
    "# Calculate the batch size\n",
    "batch_size = min(total_images, 64)  # Use 64 as the maximum batch size\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], d_loss: 0.7388325929641724, g_loss: 3.7786715030670166\n",
      "Epoch [2/100], d_loss: 0.4149091839790344, g_loss: 4.7152509689331055\n",
      "Epoch [3/100], d_loss: 0.22237707674503326, g_loss: 6.211359977722168\n",
      "Epoch [4/100], d_loss: 0.08093155920505524, g_loss: 6.7576069831848145\n",
      "Epoch [5/100], d_loss: 0.10581426322460175, g_loss: 8.078136444091797\n",
      "Epoch [6/100], d_loss: 0.046200670301914215, g_loss: 7.753810405731201\n",
      "Epoch [7/100], d_loss: 0.046498291194438934, g_loss: 7.826940536499023\n",
      "Epoch [8/100], d_loss: 0.04580950736999512, g_loss: 7.971920490264893\n",
      "Epoch [9/100], d_loss: 0.05407845228910446, g_loss: 7.725895404815674\n",
      "Epoch [10/100], d_loss: 0.04891342669725418, g_loss: 9.547094345092773\n",
      "Epoch [11/100], d_loss: 0.03687838464975357, g_loss: 9.327088356018066\n",
      "Epoch [12/100], d_loss: 0.02405150607228279, g_loss: 10.10222053527832\n",
      "Epoch [13/100], d_loss: 0.09601203352212906, g_loss: 15.178234100341797\n",
      "Epoch [14/100], d_loss: 0.009303383529186249, g_loss: 6.070601940155029\n",
      "Epoch [15/100], d_loss: 0.008545899763703346, g_loss: 17.854963302612305\n",
      "Epoch [16/100], d_loss: 0.005536513868719339, g_loss: 14.706487655639648\n",
      "Epoch [17/100], d_loss: 0.3270327150821686, g_loss: 15.31442642211914\n",
      "Epoch [18/100], d_loss: 0.011516403406858444, g_loss: 15.26413345336914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m latent_vectors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m fake_outputs \u001b[38;5;241m=\u001b[39m discriminator(fake_images)\n\u001b[0;32m     25\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m criterion(fake_outputs, real_labels)  \u001b[38;5;66;03m# Use real labels for generator loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prajw\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for real_images in dataloader:\n",
    "        # Train the discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        batch_size = real_images.size(0)\n",
    "        real_labels = torch.ones(batch_size, 1, 1, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1, 1, 1)\n",
    "        real_outputs = discriminator(real_images)\n",
    "        real_loss = criterion(real_outputs, real_labels)\n",
    "        latent_vectors = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "        fake_images = generator(latent_vectors)\n",
    "        fake_outputs = discriminator(fake_images.detach())\n",
    "        fake_loss = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        g_optimizer.zero_grad()\n",
    "        latent_vectors = torch.randn(batch_size, latent_dim, 1, 1)\n",
    "        fake_images = generator(latent_vectors)\n",
    "        fake_outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(fake_outputs, real_labels)  # Use real labels for generator loss\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "    # Print losses and save generated images\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            latent_vectors = torch.randn(16, latent_dim, 1, 1)\n",
    "            fake_images = generator(latent_vectors)\n",
    "            image_path = os.path.join('gan_images', f'generated_image_epoch_{epoch+1}.png')\n",
    "            save_image(fake_images, image_path, normalize=True)\n",
    "\n",
    "# Save the trained generator model\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
